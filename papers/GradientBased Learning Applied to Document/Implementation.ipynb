{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4073af6c-3998-48d1-b569-c5682cddda13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5809498-a749-41dc-9a72-b7b3dec867d6",
   "metadata": {},
   "source": [
    "MNIST dataset have images that have size (28,28) therefore we resize them into (32,32). We also Normalize pixels from [0,1] to [-1,1]. We do that so we can get negative gradients. If we had only [0,1] then gradient would always be positive and it would screw with our optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68dc39ce-a025-4443-9310-b5df9f9a64f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a2ac27e-5a8d-40d1-8b2d-a028ef90f75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 9.91M/9.91M [00:02<00:00, 4.64MB/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 233kB/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1.65M/1.65M [00:00<00:00, 1.91MB/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4.54k/4.54k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transform, \n",
    "                                           download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "198e6315-ddac-4938-a062-aec99cd3c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fad5cd7-0595-4a9e-9463-d543b8fa3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = SHUFFLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "831b30b0-79d1-499c-9340-52ef84f46a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozmiar obrazków = torch.Size([64, 1, 32, 32])\n",
      "Rozmiar obrazków = torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(dataloader)\n",
    "img, label = next(data_iter)\n",
    "print(f'Rozmiar obrazków = {img.shape}')\n",
    "print(f'Rozmiar obrazków = {label.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934fa955-4f67-4410-8ee2-c3b222402c84",
   "metadata": {},
   "source": [
    "For a sake of simplicity i will hardcode the layers sizes etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6171bc4-99c9-42e6-b2a0-faa506c4fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.C1 = nn.Conv2d(in_channels = 1,out_channels = 6, kernel_size = (5,5))\n",
    "        self.S2 = nn.AvgPool2d(kernel_size=(2,2),stride=2)\n",
    "        self.C3 = nn.Conv2d(in_channels = 6,out_channels = 16, kernel_size = (5,5))\n",
    "        self.S4 = nn.AvgPool2d(kernel_size = (2,2), stride = 2)\n",
    "        self.L5 = nn.Linear(400,120)\n",
    "        self.L6 = nn.Linear(120,84)\n",
    "        self.Output = nn.Linear(84,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.C1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.S2(x)\n",
    "\n",
    "        x = self.C3(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.S4(x)\n",
    "\n",
    "        x = x.view(-1,400)\n",
    "\n",
    "        x = self.L5(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.L6(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.Output(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4abcb773-fd28-453d-bce3-dbdc7c28ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abf61b5-aaa2-4802-bf92-9a981f268a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
