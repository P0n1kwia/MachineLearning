{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f9ab52-4f3f-4bcf-9ccc-8defe15b546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import math\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7ddd6d-153d-475b-96b9-092f16e9f604",
   "metadata": {},
   "source": [
    "Download dataset from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d769a-b448-4035-b260-8587c6ed53f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "words = brown.words()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4063b12-f615-4430-85fe-74948b7d4475",
   "metadata": {},
   "source": [
    "Configuring pytorch so that iw will use cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c5fb5fd-59c1-4d33-9dcd-9e303785fc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eadf6ff-d2f1-40da-a5db-7226b4a04dd7",
   "metadata": {},
   "source": [
    "Transforming all words into lower case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4d8a566-ac41-4af9-b4f7-822b746e7920",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = [w.lower() for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d66fd59-7c95-4aef-a034-9eaa71b4db82",
   "metadata": {},
   "source": [
    "Creating dictionary of unique words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "519e23dd-c42f-4f52-a700-7cfc40b49845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49815"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_dict = {i : s for s,i in enumerate(transformed)}\n",
    "unique_words = list(set(words_dict))\n",
    "len(unique_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b838a13-9bfa-4c6c-8b0b-0ce3c4375cac",
   "metadata": {},
   "source": [
    "Counting words for the first 800000  - same as paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aca929b-0e60-4599-b7c3-beedf59da9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = dict()\n",
    "n1 = 800000 #training size\n",
    "n2 = 200000 #validation size\n",
    "n3 = len(words) - (n1+n2) # testing ~161k\n",
    "for w in words[:n1]:\n",
    "    words_count[w] = words_count.get(w,0)+1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cf6f93-9eaa-4ca7-b7cb-d439862f18bc",
   "metadata": {},
   "source": [
    "We create a dictionary that stores all words that occured at least 3 times, key is a word and value is just an index, where indexes are continous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ba66c5e-d131-471b-8b0b-693446b16edd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "final_words = dict()\n",
    "idx = 0\n",
    "for w in words_count:\n",
    "    if words_count[w] >3:\n",
    "        final_words[w] = idx\n",
    "        idx +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176be5cc-a786-41b3-898b-37425a616dd1",
   "metadata": {},
   "source": [
    "We map all words that made it into final_words dictionary to unique id, where we reserved for <UNK> id = 0. <UNK> is just for us to combine all words that occured less than 3 times in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11f3b737-0c33-49a1-9770-e1d5cbc2f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {s:i+1 for i,s in enumerate(final_words)}\n",
    "word_to_id['<UNK>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c59fc5f1-4a70-45dd-931e-a6a13a9e075d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14113\n",
      "14114\n"
     ]
    }
   ],
   "source": [
    "print(max(word_to_id.values()))\n",
    "print(len(word_to_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4777b4b0-9944-45d5-81d7-55922cd8c568",
   "metadata": {},
   "source": [
    "We create function that returns list of all indexes, ie id = 1 means word the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d1cd4e9-ed4f-48cd-bf02-70e9b21eb50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 1,\n",
       " 25,\n",
       " 26,\n",
       " 5,\n",
       " 27,\n",
       " 0,\n",
       " 0,\n",
       " 19,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 9,\n",
       " 28,\n",
       " 13,\n",
       " 32,\n",
       " 15,\n",
       " 37,\n",
       " 28,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 9,\n",
       " 28,\n",
       " 29,\n",
       " 9,\n",
       " 41,\n",
       " 18,\n",
       " 42,\n",
       " 28,\n",
       " 43,\n",
       " 27,\n",
       " 33,\n",
       " 28,\n",
       " 13,\n",
       " 44,\n",
       " 45,\n",
       " 24,\n",
       " 1,\n",
       " 0,\n",
       " 46,\n",
       " 25,\n",
       " 34,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 2,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 0,\n",
       " 0,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 9,\n",
       " 56,\n",
       " 15,\n",
       " 21,\n",
       " 18,\n",
       " 27,\n",
       " 28,\n",
       " 0,\n",
       " 12,\n",
       " 33,\n",
       " 44,\n",
       " 57,\n",
       " 49,\n",
       " 0,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 24,\n",
       " 15,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 9,\n",
       " 65,\n",
       " 55,\n",
       " 44,\n",
       " 66,\n",
       " 18,\n",
       " 32,\n",
       " 28,\n",
       " 25,\n",
       " 5,\n",
       " 32,\n",
       " 15,\n",
       " 67,\n",
       " 28,\n",
       " 68,\n",
       " 69,\n",
       " 27,\n",
       " 28,\n",
       " 13,\n",
       " 32,\n",
       " 28,\n",
       " 70,\n",
       " 9,\n",
       " 71,\n",
       " 39,\n",
       " 28,\n",
       " 72,\n",
       " 9,\n",
       " 73,\n",
       " 74,\n",
       " 18,\n",
       " 24,\n",
       " 1,\n",
       " 25,\n",
       " 5,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 19,\n",
       " 78,\n",
       " 9,\n",
       " 79,\n",
       " 80,\n",
       " 39,\n",
       " 13,\n",
       " 81,\n",
       " 15,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 39,\n",
       " 86,\n",
       " 87,\n",
       " 18,\n",
       " 24,\n",
       " 88,\n",
       " 89,\n",
       " 19,\n",
       " 2,\n",
       " 90,\n",
       " 91,\n",
       " 15,\n",
       " 53,\n",
       " 92,\n",
       " 93,\n",
       " 81,\n",
       " 94,\n",
       " 39,\n",
       " 95,\n",
       " 53,\n",
       " 28,\n",
       " 96,\n",
       " 9,\n",
       " 97,\n",
       " 39,\n",
       " 98,\n",
       " 99,\n",
       " 18,\n",
       " 24,\n",
       " 1,\n",
       " 100,\n",
       " 25,\n",
       " 101,\n",
       " 102,\n",
       " 62,\n",
       " 70,\n",
       " 9,\n",
       " 103,\n",
       " 104,\n",
       " 32,\n",
       " 105,\n",
       " 99,\n",
       " 28,\n",
       " 41,\n",
       " 39,\n",
       " 2,\n",
       " 3,\n",
       " 106,\n",
       " 107,\n",
       " 33,\n",
       " 75,\n",
       " 5,\n",
       " 15,\n",
       " 82,\n",
       " 108,\n",
       " 109,\n",
       " 39,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 33,\n",
       " 0,\n",
       " 53,\n",
       " 28,\n",
       " 114,\n",
       " 69,\n",
       " 9,\n",
       " 115,\n",
       " 116,\n",
       " 18,\n",
       " 24,\n",
       " 0,\n",
       " 117,\n",
       " 118,\n",
       " 32,\n",
       " 28,\n",
       " 25,\n",
       " 5,\n",
       " 75,\n",
       " 119,\n",
       " 15,\n",
       " 93,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 53,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 39,\n",
       " 128,\n",
       " 28,\n",
       " 129,\n",
       " 9,\n",
       " 130,\n",
       " 18,\n",
       " 24,\n",
       " 1,\n",
       " 29,\n",
       " 0,\n",
       " 131,\n",
       " 32,\n",
       " 28,\n",
       " 25,\n",
       " 5,\n",
       " 32,\n",
       " 15,\n",
       " 132,\n",
       " 133,\n",
       " 27,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 62,\n",
       " 138,\n",
       " 9,\n",
       " 74,\n",
       " 136,\n",
       " 139,\n",
       " 18,\n",
       " 24,\n",
       " 88,\n",
       " 140,\n",
       " 19,\n",
       " 28,\n",
       " 74,\n",
       " 15,\n",
       " 141,\n",
       " 142,\n",
       " 53,\n",
       " 143,\n",
       " 18,\n",
       " 73,\n",
       " 144,\n",
       " 24,\n",
       " 0,\n",
       " 9,\n",
       " 79,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 44,\n",
       " 148,\n",
       " 89,\n",
       " 49,\n",
       " 28,\n",
       " 149,\n",
       " 25,\n",
       " 24,\n",
       " 88,\n",
       " 140,\n",
       " 19,\n",
       " 28,\n",
       " 150,\n",
       " 151,\n",
       " 15,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 39,\n",
       " 0,\n",
       " 28,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 19,\n",
       " 7,\n",
       " 158,\n",
       " 159,\n",
       " 9,\n",
       " 28,\n",
       " 147,\n",
       " 160,\n",
       " 123,\n",
       " 161,\n",
       " 18,\n",
       " 24,\n",
       " 1,\n",
       " 100,\n",
       " 25,\n",
       " 22,\n",
       " 62,\n",
       " 0,\n",
       " 162,\n",
       " 28,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 9,\n",
       " 167,\n",
       " 154,\n",
       " 168,\n",
       " 42,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 27,\n",
       " 172,\n",
       " 173,\n",
       " 24,\n",
       " 15,\n",
       " 174,\n",
       " 132,\n",
       " 175,\n",
       " 9,\n",
       " 28,\n",
       " 176,\n",
       " 177,\n",
       " 27,\n",
       " 28,\n",
       " 2,\n",
       " 3,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 18,\n",
       " 32,\n",
       " 28,\n",
       " 25,\n",
       " 5,\n",
       " 32,\n",
       " 181,\n",
       " 28,\n",
       " 163,\n",
       " 164,\n",
       " 131,\n",
       " 15,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 53,\n",
       " 185,\n",
       " 93,\n",
       " 154,\n",
       " 186,\n",
       " 28,\n",
       " 170,\n",
       " 107,\n",
       " 9,\n",
       " 187,\n",
       " 28,\n",
       " 188,\n",
       " 27,\n",
       " 28,\n",
       " 189,\n",
       " 190,\n",
       " 28,\n",
       " 191,\n",
       " 9,\n",
       " 2,\n",
       " 3,\n",
       " 32,\n",
       " 33,\n",
       " 192,\n",
       " 193,\n",
       " 9,\n",
       " 73,\n",
       " 194,\n",
       " 24,\n",
       " 1,\n",
       " 195,\n",
       " 5,\n",
       " 196,\n",
       " 197,\n",
       " 15,\n",
       " 62,\n",
       " 198,\n",
       " 199,\n",
       " 9,\n",
       " 93,\n",
       " 154,\n",
       " 200,\n",
       " 0,\n",
       " 73,\n",
       " 180,\n",
       " 27,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 188,\n",
       " 18,\n",
       " 24,\n",
       " 204,\n",
       " 32,\n",
       " 15,\n",
       " 205,\n",
       " 206,\n",
       " 19,\n",
       " 27,\n",
       " 28,\n",
       " 207,\n",
       " 2,\n",
       " 3,\n",
       " 122,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 9,\n",
       " 93,\n",
       " 211,\n",
       " 154,\n",
       " 18,\n",
       " 32,\n",
       " 28,\n",
       " 195,\n",
       " 5,\n",
       " 24,\n",
       " 15,\n",
       " 212,\n",
       " 53,\n",
       " 213,\n",
       " 73,\n",
       " 214,\n",
       " 215,\n",
       " 53,\n",
       " 23,\n",
       " 62,\n",
       " 0,\n",
       " 216,\n",
       " 18,\n",
       " 102,\n",
       " 2,\n",
       " 217,\n",
       " 24,\n",
       " 1,\n",
       " 25,\n",
       " 148,\n",
       " 101,\n",
       " 102,\n",
       " 28,\n",
       " 2,\n",
       " 0,\n",
       " 218,\n",
       " 33,\n",
       " 182,\n",
       " 47,\n",
       " 219,\n",
       " 220,\n",
       " 42,\n",
       " 221,\n",
       " 113,\n",
       " 27,\n",
       " 28,\n",
       " 222,\n",
       " 9,\n",
       " 0,\n",
       " 32,\n",
       " 223,\n",
       " 39,\n",
       " 224,\n",
       " 39,\n",
       " 28,\n",
       " 0,\n",
       " 9,\n",
       " 225,\n",
       " 39,\n",
       " 226,\n",
       " 24,\n",
       " 0,\n",
       " 227,\n",
       " 1,\n",
       " 25,\n",
       " 5,\n",
       " 75,\n",
       " 228,\n",
       " 28,\n",
       " 218,\n",
       " 15,\n",
       " 182,\n",
       " 229,\n",
       " 230,\n",
       " 221,\n",
       " 231,\n",
       " 232,\n",
       " 28,\n",
       " 233,\n",
       " 18,\n",
       " 9,\n",
       " 120,\n",
       " 234,\n",
       " 100,\n",
       " 0,\n",
       " 32,\n",
       " 28,\n",
       " 41,\n",
       " 235,\n",
       " 236,\n",
       " 39,\n",
       " 7,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 24,\n",
       " 15,\n",
       " 240,\n",
       " 241,\n",
       " 122,\n",
       " 242,\n",
       " 53,\n",
       " 243,\n",
       " 27,\n",
       " 244,\n",
       " 39,\n",
       " 27,\n",
       " 245,\n",
       " 28,\n",
       " 0,\n",
       " 0,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 39,\n",
       " 221,\n",
       " 249,\n",
       " 39,\n",
       " 250,\n",
       " 251,\n",
       " 246,\n",
       " 0,\n",
       " 252,\n",
       " 18,\n",
       " 32,\n",
       " 28,\n",
       " 25,\n",
       " 5,\n",
       " 24,\n",
       " 0,\n",
       " 10,\n",
       " 253,\n",
       " 0,\n",
       " 254,\n",
       " 32,\n",
       " 28,\n",
       " 25,\n",
       " 89,\n",
       " 15,\n",
       " 19,\n",
       " 255,\n",
       " 28,\n",
       " 253,\n",
       " 256,\n",
       " 257,\n",
       " 36,\n",
       " 258,\n",
       " 259,\n",
       " 28,\n",
       " 254,\n",
       " 123,\n",
       " 109,\n",
       " 27,\n",
       " 62,\n",
       " 43,\n",
       " 19,\n",
       " 214,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 18,\n",
       " 24,\n",
       " 1,\n",
       " 25,\n",
       " 76,\n",
       " 263,\n",
       " 264,\n",
       " 32,\n",
       " 181,\n",
       " 75,\n",
       " 265,\n",
       " 19,\n",
       " 15,\n",
       " 266,\n",
       " 122,\n",
       " 123,\n",
       " 267,\n",
       " 268,\n",
       " 9,\n",
       " 28,\n",
       " 269,\n",
       " 113,\n",
       " 9,\n",
       " 28,\n",
       " 270,\n",
       " 42,\n",
       " 28,\n",
       " 271,\n",
       " 9,\n",
       " 272,\n",
       " 28,\n",
       " 273,\n",
       " 274,\n",
       " 18,\n",
       " 24,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 103,\n",
       " 279,\n",
       " 32,\n",
       " 28,\n",
       " 25,\n",
       " 89,\n",
       " 19,\n",
       " 280,\n",
       " 281,\n",
       " 259,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 277,\n",
       " 123,\n",
       " 285,\n",
       " 162,\n",
       " 28,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 39,\n",
       " 15,\n",
       " 62,\n",
       " 286,\n",
       " 32,\n",
       " 287,\n",
       " 0,\n",
       " 84,\n",
       " 0,\n",
       " 123,\n",
       " 285,\n",
       " 42,\n",
       " 288,\n",
       " 39,\n",
       " 289,\n",
       " 290,\n",
       " 162,\n",
       " 28,\n",
       " 276,\n",
       " 18,\n",
       " 24,\n",
       " 281,\n",
       " 291,\n",
       " 282,\n",
       " 2,\n",
       " 90,\n",
       " 15,\n",
       " 292,\n",
       " 190,\n",
       " 74,\n",
       " 293,\n",
       " 53,\n",
       " 294,\n",
       " 153,\n",
       " 295,\n",
       " 19,\n",
       " 214,\n",
       " 296,\n",
       " 28,\n",
       " 297,\n",
       " 9,\n",
       " 62,\n",
       " 298,\n",
       " 39,\n",
       " 299,\n",
       " 18,\n",
       " 300,\n",
       " 301,\n",
       " 42,\n",
       " 74,\n",
       " 302,\n",
       " 24,\n",
       " 1,\n",
       " 25,\n",
       " 303,\n",
       " 28,\n",
       " 130,\n",
       " 39,\n",
       " 304,\n",
       " 9,\n",
       " 28,\n",
       " 41,\n",
       " 305,\n",
       " 131,\n",
       " 32,\n",
       " 28,\n",
       " 2,\n",
       " 306,\n",
       " 0,\n",
       " 307,\n",
       " 32,\n",
       " 28,\n",
       " 0,\n",
       " 39,\n",
       " 0,\n",
       " 308,\n",
       " 309,\n",
       " 32,\n",
       " 310,\n",
       " 311,\n",
       " 39,\n",
       " 28,\n",
       " 2,\n",
       " 312,\n",
       " 131,\n",
       " 24,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 42,\n",
       " 319,\n",
       " 246,\n",
       " 320,\n",
       " 321,\n",
       " 32,\n",
       " 322,\n",
       " 323,\n",
       " 316,\n",
       " 32,\n",
       " 27,\n",
       " 2,\n",
       " 50,\n",
       " 51,\n",
       " 6,\n",
       " 24,\n",
       " 324,\n",
       " 325,\n",
       " 48,\n",
       " 326,\n",
       " 327,\n",
       " 24,\n",
       " 1,\n",
       " 328,\n",
       " 44,\n",
       " 329,\n",
       " 330,\n",
       " 291,\n",
       " 32,\n",
       " 331,\n",
       " 24,\n",
       " 332,\n",
       " 92,\n",
       " 62,\n",
       " 333,\n",
       " 32,\n",
       " 314,\n",
       " 334,\n",
       " 60,\n",
       " 32,\n",
       " 39,\n",
       " 62,\n",
       " 335,\n",
       " 32,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 0,\n",
       " 9,\n",
       " 339,\n",
       " 24,\n",
       " 0,\n",
       " 42,\n",
       " 28,\n",
       " 340,\n",
       " 5,\n",
       " 19,\n",
       " 7,\n",
       " 0,\n",
       " 341,\n",
       " 342,\n",
       " 182,\n",
       " 47,\n",
       " 343,\n",
       " 344,\n",
       " 24,\n",
       " 1,\n",
       " 325,\n",
       " 345,\n",
       " 28,\n",
       " 346,\n",
       " 347,\n",
       " 137,\n",
       " 15,\n",
       " 348,\n",
       " 18,\n",
       " 39,\n",
       " 320,\n",
       " 349,\n",
       " 137,\n",
       " 350,\n",
       " 24,\n",
       " 88,\n",
       " 345,\n",
       " 320,\n",
       " 351,\n",
       " 349,\n",
       " 137,\n",
       " 352,\n",
       " 39,\n",
       " 23,\n",
       " 9,\n",
       " 353,\n",
       " 137,\n",
       " 0,\n",
       " 32,\n",
       " 354,\n",
       " 24,\n",
       " 1,\n",
       " 325,\n",
       " 5,\n",
       " 19,\n",
       " 28,\n",
       " 328,\n",
       " 182,\n",
       " 263,\n",
       " 355,\n",
       " 356,\n",
       " 137,\n",
       " 357,\n",
       " 39,\n",
       " 321,\n",
       " 42,\n",
       " 358,\n",
       " 359,\n",
       " 62,\n",
       " 360,\n",
       " 24,\n",
       " 1,\n",
       " 316,\n",
       " 361,\n",
       " 132,\n",
       " 162,\n",
       " 0,\n",
       " 362,\n",
       " 363,\n",
       " 0,\n",
       " 364,\n",
       " 24,\n",
       " 365,\n",
       " 366,\n",
       " 0,\n",
       " 44,\n",
       " 345,\n",
       " 102,\n",
       " 28,\n",
       " 325,\n",
       " 137,\n",
       " 28,\n",
       " 346,\n",
       " 348,\n",
       " 24,\n",
       " 316,\n",
       " 182,\n",
       " 47,\n",
       " 340,\n",
       " 9,\n",
       " 41,\n",
       " 32,\n",
       " 190,\n",
       " 191,\n",
       " 9,\n",
       " 175,\n",
       " 367,\n",
       " 0,\n",
       " 32,\n",
       " 368,\n",
       " 369,\n",
       " 24,\n",
       " 324,\n",
       " 261,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 53,\n",
       " 320,\n",
       " 13,\n",
       " 53,\n",
       " 74,\n",
       " 373,\n",
       " 27,\n",
       " 374,\n",
       " 24,\n",
       " 1,\n",
       " 346,\n",
       " 375,\n",
       " 46,\n",
       " 9,\n",
       " 376,\n",
       " 0,\n",
       " 258,\n",
       " 259,\n",
       " 24,\n",
       " 377,\n",
       " 214,\n",
       " 123,\n",
       " 378,\n",
       " 49,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 32,\n",
       " 379,\n",
       " 380,\n",
       " 62,\n",
       " 381,\n",
       " 27,\n",
       " 28,\n",
       " 382,\n",
       " 383,\n",
       " 12,\n",
       " 384,\n",
       " 313,\n",
       " 316,\n",
       " 385,\n",
       " 19,\n",
       " 386,\n",
       " 387,\n",
       " 263,\n",
       " 388,\n",
       " 42,\n",
       " 0,\n",
       " 24,\n",
       " 389,\n",
       " 390,\n",
       " 82,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 53,\n",
       " 394,\n",
       " 62,\n",
       " 381,\n",
       " 27,\n",
       " 28,\n",
       " 395,\n",
       " 396,\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_data_ints(words):\n",
    "    ints = []\n",
    "    for w in words:\n",
    "        if w in word_to_id:\n",
    "            ints.append(word_to_id[w])\n",
    "        else:\n",
    "            ints.append(word_to_id['<UNK>'])\n",
    "    return ints\n",
    "\n",
    "train_data_ids = train_data_ints(words[:n1])\n",
    "train_data_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54da18e9-cf2e-4fc3-915d-9e06c4575288",
   "metadata": {},
   "source": [
    "Proper function to create training dataset. N_grams is just our context window ie. The cat is walking down .... means that we need n_gram = 6 to predict sixth next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53471e60-25ad-4de9-ad3f-270925f80500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, n_grams):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(len(data) - n_grams):\n",
    "        X.append(data[i:i+n_grams-1])\n",
    "        Y.append(data[i+n_grams-1])\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X,Y\n",
    "Xtrain,Ytrain= create_dataset(train_data_ids,n_grams=6)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d868f4c-1547-4c19-8072-fc0cdf6c513c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    2,    3,    4,    0],\n",
       "        [   2,    3,    4,    0,    5],\n",
       "        [   3,    4,    0,    5,    6],\n",
       "        ...,\n",
       "        [   0,    0,   32,  598,   32],\n",
       "        [   0,   32,  598,   32,  253],\n",
       "        [  32,  598,   32,  253, 6591]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb0ccd27-ec7e-4eff-92c5-29a84013c6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   5,    6,    7,  ...,  253, 6591,  456])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa067e0f-3cf9-40d3-a84c-9195a35a4037",
   "metadata": {},
   "source": [
    "Using pytorch we create our MLP. As in paper we have lookup matrix C, 1 hidden layer and output layer. In hidden layer we use tanh activation function and for last layaer we will use softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b5616ff-6391-480b-96bc-86f837c0eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class NPLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
    "        super(NPLM, self).__init__()\n",
    "        self.C = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.hidden = nn.Linear(embedding_dim*context_size,hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim,vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs to tensor o wymiarach (batch_size, context_size)\n",
    "        output_embeding = self.C(inputs)\n",
    "        x= output_embeding.view(inputs.size(0),-1)\n",
    "        output_hidden = torch.tanh(self.hidden(x))\n",
    "        out = self.output(output_hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09834ae-ac27-40b8-a959-3c029b790893",
   "metadata": {},
   "source": [
    "Creating training parameters and setting our optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1066fe46-bcba-4d35-a1c7-d68b7235aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 60    \n",
    "HIDDEN_DIM = 60   \n",
    "CONTEXT_SIZE = 5  \n",
    "VOCAB_SIZE = len(word_to_id)\n",
    "LEARNING_RATE = 0.001 \n",
    "model = NPLM(VOCAB_SIZE, EMBED_DIM, CONTEXT_SIZE, HIDDEN_DIM).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd49989d-4601-4af0-b732-9feeb7f2eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da054fa-9e68-435b-954b-b49c4160aafe",
   "metadata": {},
   "source": [
    "We use dataloader to create batches our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "850cdfe3-1eaf-47d3-9d2d-152951c716a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "dataset = TensorDataset(Xtrain, Ytrain)\n",
    "BATCH_SIZE = 64 \n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc7a55e-b481-4876-b589-047dd47afc0b",
   "metadata": {},
   "source": [
    "Perplexity is a measure of uncertainty in our model, eg how disoriented is our model, the lower the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e1de436-ab59-4c4a-8ee0-dfb876b9141f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 6.0591 | Perplexity = 427.99\n",
      "Epoch 2: Loss = 5.6093 | Perplexity = 272.94\n",
      "Epoch 3: Loss = 5.4292 | Perplexity = 227.97\n",
      "Epoch 4: Loss = 5.3043 | Perplexity = 201.19\n",
      "Epoch 5: Loss = 5.1997 | Perplexity = 181.22\n",
      "Epoch 6: Loss = 5.1122 | Perplexity = 166.03\n",
      "Epoch 7: Loss = 5.0402 | Perplexity = 154.51\n",
      "Epoch 8: Loss = 4.9732 | Perplexity = 144.49\n",
      "Epoch 9: Loss = 4.9091 | Perplexity = 135.51\n",
      "Epoch 10: Loss = 4.8511 | Perplexity = 127.88\n",
      "Epoch 11: Loss = 4.7985 | Perplexity = 121.32\n",
      "Epoch 12: Loss = 4.7500 | Perplexity = 115.59\n",
      "Epoch 13: Loss = 4.7041 | Perplexity = 110.40\n",
      "Epoch 14: Loss = 4.6637 | Perplexity = 106.03\n",
      "Epoch 15: Loss = 4.6247 | Perplexity = 101.97\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    for step, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.grad = None\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_function(pred,y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: Loss = {avg_loss:.4f} | Perplexity = {perplexity:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9d9f119-aa54-43f9-b6b6-36cdec94ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {i: w for w, i in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664dd7ab-f7a9-4d34-9ff8-4df5b717920a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f6b357b-1144-4468-b7ed-9ed1e405f8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlanta's recent primary election produced no evidence of the <UNK> . The <UNK> of the <UNK> <UNK> . The <UNK> <UNK> , <UNK> <UNK> , <UNK> , "
     ]
    }
   ],
   "source": [
    "def generate_text(model, seed,length=20):\n",
    "    model.eval()\n",
    "    words = seed.split()\n",
    "    current_ids = []\n",
    "    for w in words:\n",
    "        if w in word_to_id:\n",
    "            current_ids.append(word_to_id[w])\n",
    "        else:\n",
    "            current_ids.append(word_to_id.get('<UNK>', 0))\n",
    "    print(seed, end=' ')\n",
    "    for _ in range(length):\n",
    "        input_ids = current_ids[-CONTEXT_SIZE:]\n",
    "        x = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "        output = model(x)\n",
    "        best_word = torch.argmax(output).item()\n",
    "        word = id_to_word[best_word]\n",
    "        print(word,end=' ')\n",
    "        current_ids.append(best_word)\n",
    "\n",
    "seed = \"Atlanta's recent primary election produced no evidence\"\n",
    "generate_text(model,seed,length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4680b2cd-5954-4373-95c6-a63558e2a34b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
